# ğŸšš Supply Chain Lakehouse: End-to-End DE, ML & AI Project

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![DuckDB](https://img.shields.io/badge/Storage-DuckDB-yellow)
![dbt](https://img.shields.io/badge/Transformation-dbt-orange)
![Streamlit](https://img.shields.io/badge/Frontend-Streamlit-red)
![Status](https://img.shields.io/badge/Status-Active-success)

**A modern Data Lakehouse architecture built on a local machine using 100% open-source tools.**

This project demonstrates an end-to-end data pipeline that ingests raw supply chain data, transforms it using dbt, stores it in DuckDB, predicts delivery delays using Machine Learning, and visualizes insights via an interactive Streamlit dashboard equipped with LLM capabilities.

---

## ğŸ§± Architecture Overview

The pipeline follows a "Modern Data Stack" approach tailored for local development:

```mermaid
graph LR
    A[Raw CSV Data] -->|Python Script| B[(DuckDB Raw Layer)]
    B -->|dbt Core| C[Staging, Dims & Facts]
    C -->|dbt Core| D[Data Marts]
    D -->|Query| E[Streamlit Dashboard]
    D -->|Training Data| F[Scikit-Learn Model]
    F -->|Predictions| E
    G[Gemini/GPT API] -.->|Insights| E


## ğŸ“‚ Project StructurePlaintextsupply-chain-lakehouse/
â”‚
â”œâ”€â”€ data/                       # Raw input data
â”‚   â””â”€â”€ raw_supply_chain.csv
â”‚
â”œâ”€â”€ warehouse/                  # OLAP Database file
â”‚   â””â”€â”€ supply_chain.duckdb
â”‚
â”œâ”€â”€ scripts/                    # ETL & Training Scripts
â”‚   â”œâ”€â”€ ingest_csv_to_duckdb.py
â”‚   â”œâ”€â”€ inspect_duckdb.py
â”‚   â””â”€â”€ train_model.py
â”‚
â”œâ”€â”€ dbt/                        # dbt Transformation Project
â”‚   â””â”€â”€ supply_chain/
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ staging/        # Cleaned data
â”‚       â”‚   â”œâ”€â”€ dimensions/     # Dimension tables (Product, etc)
â”‚       â”‚   â”œâ”€â”€ facts/          # Fact tables (Sales)
â”‚       â”‚   â””â”€â”€ marts/          # Aggregated tables for BI
â”‚       â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ supply_chain_app/           # Dashboard Application
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ ml.py
â”‚   â”‚   â””â”€â”€ charts.py
â”‚   â”œâ”€â”€ models/                 # Serialized ML Models
â”‚   â”‚   â””â”€â”€ delay_predictor.pkl
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ README.md
ğŸ§ª DatasetThe project uses the Supply Chain Dataset (Cosmetics & Logistics) sourced from Kaggle.Records: ~100 rows (Simulated data)Key Features: Product SKU, Price, Lead times, Shipping costs, Defect rates, Routes, and Carrier details.ğŸš€ Getting StartedFollow these steps to run the project from scratch.1ï¸âƒ£ Clone the RepositoryBashgit clone [https://github.com/talhazulfakhri/lakehouse-dbt-duckdb.git](https://github.com/talhazulfakhri/lakehouse-dbt-duckdb.git)
cd supply-chain-lakehouse
2ï¸âƒ£ Environment SetupCreate a virtual environment to keep dependencies clean.Bashpython -m venv .venv
# Activate: Windows
.venv\Scripts\activate
# Activate: Mac/Linux
source .venv/bin/activate

# Install dependencies
pip install -r supply_chain_app/requirements.txt
3ï¸âƒ£ Ingest Data (ETL)Load the raw CSV into the DuckDB raw layer.Bashpython scripts/ingest_csv_to_duckdb.py
Output: Creates warehouse/supply_chain.duckdb populated with raw_supply_chain.4ï¸âƒ£ Run dbt TransformationsClean, model, and aggregate the data.Bashcd dbt/supply_chain
dbt deps
dbt run
Output: Generates stg_supply_chain, dim_product, fact_sales, and mart_supply_chain_performance.5ï¸âƒ£ Train Machine Learning ModelTrain a Random Forest Classifier to predict shipping delays.Bash# Go back to root if inside dbt folder
cd ../../
python scripts/train_model.py
Features: supplier_lead_time, defect_rate, shipping_costTarget: delay_flag (Derived from shipping days)Output: Saves model to supply_chain_app/models/delay_predictor.pkl6ï¸âƒ£ Launch the DashboardStart the Streamlit app to visualize the data.Bashcd supply_chain_app
streamlit run app.py
ğŸ“Š Analytics & InsightsThe Dashboard provides three main layers of value:Business Intelligence (BI):Sales KPIs & Revenue analysis.Defect rate vs. Manufacturing cost correlation.Top performing products and routes.Predictive Analytics (ML):Real-time prediction of shipment delays based on lead time and carrier performance.Generative AI (Ready):Architecture supports plugging in OpenAI/Gemini API to query the DuckDB warehouse using Natural Language.
